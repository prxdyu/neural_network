{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbm6xijgwZmH"
   },
   "source": [
    "### Implementation of Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "The implemented neural network, named NN, is a feedforward neural network capable of handling classification tasks. The key features of this implementation include:\n",
    "\n",
    "- Architecture: \n",
    "The neural network architecture is specified by the number of input features (n_features), the number of output classes (n_classes), and the number of neurons in the hidden layer (n_hidden).\n",
    "\n",
    "- Activation Functions: ReLU (Rectified Linear Unit) activation is applied to the hidden layer, and softmax activation is applied to the output layer.\n",
    "\n",
    "- Loss Function: The cross-entropy loss function is used for training the neural network.\n",
    "\n",
    "- Regularization: L2 regularization is incorporated to prevent overfitting during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a class for Neural Network\n",
    "class NN:\n",
    "\n",
    "   \"\"\"\n",
    "   n_features: no of features\n",
    "   n_classes: no of classes (no of output neurons)\n",
    "   n_hidden: no of neurons in the hidden layers\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   def __init__(self,n_features,n_classes,n_hidden):\n",
    "\n",
    "      self.d=n_features\n",
    "      self.n=n_classes\n",
    "      self.h=n_hidden\n",
    "\n",
    "      #creating the weight matrices W1 (collection of weight values from input layer to hidden layer) of dimension (dxh) each column is a weight vectors for each neuron\n",
    "      self.W1=0.01*np.random.randn(self.d,self.h)\n",
    "\n",
    "      #creating a bias matrix b1 (collection of bias values from input layer to hidden layer) of dimension (1xh)\n",
    "      self.b1 = np.zeros((1,self.h))\n",
    "\n",
    "      #creating the weight matrices W2 (collection of weight values from hidden layer to output layer) of dimension (hxn) each column is a weight vectors for each neuron\n",
    "\n",
    "      self.W2=0.01*np.random.randn(self.h,self.n)\n",
    "\n",
    "      #creating a bias matrix b2 (collection of bias values from hidden layer to output layer) of dimension (1xn)\n",
    "      self.b2 = np.zeros((1,self.n))\n",
    "\n",
    "   def frwd_prop(self,x):\n",
    "\n",
    "      # multiplying the weight with the values(datapoint) and adding the bias term b1\n",
    "      z1=np.dot(x,self.W1)+self.b1\n",
    "\n",
    "      # applying the relu function to z1\n",
    "      A1=np.maximum(0,z1)\n",
    "\n",
    "      # multiplying the weight with the values (r1) and adding the bias term b2\n",
    "      z2=np.dot(A1,self.W2)+self.b2\n",
    "\n",
    "\n",
    "      # applying the softmax to the z2\n",
    "      A2=np.exp(z2)\n",
    "      A2=A2/np.sum(A2,axis=1,keepdims=True)\n",
    "\n",
    "      return A1,A2\n",
    "\n",
    "   def ce_loss(self,y_true,y_pred_proba):\n",
    "\n",
    "          # computing the cross entropy loss\n",
    "          num_examples=y.shape[0]\n",
    "          yij_pij=-np.log(y_pred_proba[range(num_examples),y])\n",
    "          loss=np.sum(yij_pij)/num_examples\n",
    "          return loss\n",
    "\n",
    "   def backward_prop(self,x,y,A1,A2):\n",
    "\n",
    "      # capturing the no of datapoints\n",
    "      num_examples=y.shape[0]\n",
    "\n",
    "      # computing the derivatives of CE loss wrt to z(inputs to sfmx layer)\n",
    "      \"\"\" derivative of CE loss wrt to zj  dL/dzj= Pij-Yij \"\"\"\n",
    "      dZ2 =A2\n",
    "      dZ2[range(num_examples),y] -= 1\n",
    "\n",
    "      # normalizing the gradients\n",
    "      dZ2 /= num_examples\n",
    "      # computing the derivative of loss wrt W2)\n",
    "      dW2=np.dot(A1.T,dZ2)\n",
    "      # computing the derivative of loss wrt b2\n",
    "      db2=np.sum(dZ2,axis=0,keepdims=True)\n",
    "\n",
    "      # computing the derivative of loss wrt A1\n",
    "      dA1=np.dot(dZ2,self.W2.T)\n",
    "\n",
    "      # computing the gradient for ReLu (gradient is 0 for the negative points)\n",
    "      dA1[dA1<0]==0\n",
    "\n",
    "      # computing the gradient for z1\n",
    "      dZ1=dA1\n",
    "\n",
    "      # computing the gradient for W1\n",
    "      dW1=np.dot(x.T,dZ1)\n",
    "\n",
    "      # computing the gradient for b2\n",
    "      db1=np.sum(dZ1,axis=0,keepdims=True)\n",
    "\n",
    "      return dW1, db1, dW2, db2\n",
    "\n",
    "\n",
    "\n",
    "   def fit(self,x,y,reg,max_iters,eta):\n",
    "\n",
    "      num_examples=x.shape[0]\n",
    "\n",
    "      # doing forward and backward prop max_iter times\n",
    "      for i in range(max_iters):\n",
    "\n",
    "          #forward prop\n",
    "          A1,A2=self.frwd_prop(x)\n",
    "\n",
    "          #calculating the loss\n",
    "          loss=self.ce_loss(y,A2)\n",
    "          # calculating the regularization loss\n",
    "          reg_loss = 0.5*reg*np.sum(self.W1*self.W1) + 0.5*reg*np.sum(self.W2*self.W2)\n",
    "          # computing the total loss\n",
    "          total_loss=loss+reg_loss\n",
    "\n",
    "          if i % 1000 == 0:\n",
    "                print(\"iteration %d: loss %f\" % (i, total_loss))\n",
    "\n",
    "          # backprop\n",
    "          dW1, db1, dW2, db2  = self.backward_prop(x,y,A1,A2)\n",
    "\n",
    "          # during the backprop we have computed the gradients only with respect to loss, not regularization.\n",
    "          # add regularization gradient contribution\n",
    "          dW2 += reg * self.W2\n",
    "          dW1 += reg * self.W1\n",
    "\n",
    "          # updating the parameters\n",
    "          self.W1+= -eta*dW1\n",
    "          self.W2+= -eta*dW2\n",
    "          self.b1+= -eta*db1\n",
    "          self.b2+= -eta*db2\n",
    "\n",
    "\n",
    "   def predict(self,x):\n",
    "\n",
    "      # doing foward prop\n",
    "      _,y_pred=self.frwd_prop(x)\n",
    "\n",
    "      # converting the  class probabilities into class labels\n",
    "      y_pred=np.argmax(y_pred,axis=1)\n",
    "\n",
    "      return y_pred\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
